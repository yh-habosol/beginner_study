{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "5                5.4               3.9                1.7               0.4   \n",
       "6                4.6               3.4                1.4               0.3   \n",
       "7                5.0               3.4                1.5               0.2   \n",
       "8                4.4               2.9                1.4               0.2   \n",
       "9                4.9               3.1                1.5               0.1   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "5      0  \n",
       "6      0  \n",
       "7      0  \n",
       "8      0  \n",
       "9      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##붓꽃 데잍터셋 로드\n",
    "iris = load_iris()\n",
    "\n",
    "#데이터 세트에서 피처만으로 된 데이터를 넘파이로 가지고 있음.\n",
    "iris_data = iris.data\n",
    "\n",
    "#iris.target은 붓꽃 데이터 세트에서 레이블 데이터를 넘파이로 가지고 있음.\n",
    "iris_label = iris.target\n",
    "print(iris_label)\n",
    "print(iris.target_names)\n",
    "\n",
    "iris_df = pd.DataFrame(data = iris_data, columns = iris.feature_names)\n",
    "iris_df['label'] = iris_label\n",
    "iris_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9667\n"
     ]
    }
   ],
   "source": [
    "##학습데이터, 테스트 데이터로 분리하기\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size = 0.2, random_state = 1)\n",
    "\n",
    "##첫번째 인자는 피처 데이트 세트, 두번째 인자는 레이블 데이터 세트입니다. test_size는 훈련,테스트 셋의 비율을 나타냅니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##객체 생성\n",
    "df_clf = DecisionTreeClassifier(random_state = 11)\n",
    "\n",
    "##fit 메소드를 이용해 학습\n",
    "df_clf.fit(X_train, y_train)\n",
    "\n",
    "##테스트 데이터에 대한 예측값 반환\n",
    "pred = df_clf.predict(X_test)\n",
    "\n",
    "##정확도 비교\n",
    "print(\"accuracy: {0:.4f}\".format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##과정 정리\n",
    "'''\n",
    "1. 데이터 세트 분리\n",
    "2. 학습 데이터를 기반으로 머신러닝 알고리즘 적용하여 학습\n",
    "3. 학습된 모델을 이용하여 테스트 데이터분류 예측\n",
    "4. 정확도를 비교하여 모델 성능 평가\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지도학습의 두 축인 분류와 회귀의 다양한 알고리즘을 구현한 모든 사이킷런 클래스는 fit, predict만을 이용해\n",
    "간단히 학습과 예측 결과를 반환합니다.\n",
    "분류는 Classifier, 회귀는 Regressor로 지칭합니다.\n",
    "두 클래스를 합친 클래스를 Estimator라고 합니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##사이킷런 내장 데이터셋은 딕셔너리 형태로 되어있습니다.\n",
    "iris_data = load_iris()\n",
    "iris_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data는 피처의 데이터 세트를 가리킵니다.\n",
    "target는 분류 시 레이블 값, 회귀일 때는 숫자 결과값 데이터 세트입니다.\n",
    "target_names는 개별 레이블의 이름입니다.\n",
    "feature_names는 피처의 이름을 나타냅니다.\n",
    "DESCR은 데이터 세트에 대한 설명과 각 피처의 설명을 나타냅니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "모델이 학습데이터에만 과도하게 최적화되어 실제 예측을 다른 데이터로 수행할 때 예측 성능이 과도하게 떨어지는 overfitting이 발생할\n",
    "수 있습니다. 이를 개선하기 위해 교차 검증을 이용합니다.\n",
    "교차 검증은 학습 데이터를 다시 분할하여 학습데이터와 검증 데이터로 나눈 후, 1차적으로 평가가 끝난 뒤, 테스트 데이터로 평가를 \n",
    "하는 것을 말합니다.\n",
    "\n",
    "K 폴드 교차 검증은 가장 보편적으로 사용되는 교차 검증 기법입니다.\n",
    "K개의 데이터 폴드 세트를 만들어서 K번만큼 각 폴데 세트에 학습과 검증 평가를 반복적으로 수행하는 방법입니다.\n",
    "그 후 전체 결과를 평균한 결과를 가지고 예측 성능을 평가합니다.\n",
    "사이킷런에서는 KFold와 StratifiedKFold클래스를 통해 K 폴드 교차 검증 프로세스를 구현합니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 세트 크기:  150\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "features  = iris.data\n",
    "label = iris.target\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state = 156)\n",
    "\n",
    "#5개의 폴드 세트로 분리하는 KFold 객체와 폴드 세트별 정확도를 담을 리스트 객체 생성\n",
    "kfold = KFold(n_splits = 5)\n",
    "cv_accuracy = []\n",
    "print(\"데이터 세트 크기: \", features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#1 교차 검증 정확도: 1.0, 학습 데이터 크기: 120, 검증 데이터 크기: 30\n",
      "\n",
      "#1 검증 세트 인덱스: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "#2 교차 검증 정확도: 1.0, 학습 데이터 크기: 120, 검증 데이터 크기: 30\n",
      "\n",
      "#2 검증 세트 인덱스: [30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53\n",
      " 54 55 56 57 58 59]\n",
      "\n",
      "#3 교차 검증 정확도: 0.9667, 학습 데이터 크기: 120, 검증 데이터 크기: 30\n",
      "\n",
      "#3 검증 세트 인덱스: [60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83\n",
      " 84 85 86 87 88 89]\n",
      "\n",
      "#4 교차 검증 정확도: 1.0, 학습 데이터 크기: 120, 검증 데이터 크기: 30\n",
      "\n",
      "#4 검증 세트 인덱스: [ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119]\n",
      "\n",
      "#5 교차 검증 정확도: 1.0, 학습 데이터 크기: 120, 검증 데이터 크기: 30\n",
      "\n",
      "#5 검증 세트 인덱스: [120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149]\n",
      "평균 정확됴:  0.9933399999999999\n"
     ]
    }
   ],
   "source": [
    "##KFold객체의 split함수를 호출하여 전체 데이터를 폴드 데이터 세트로 분리할 수 있습니다. 이대 split함수는 분할된 데이터의 인덱스를 \n",
    "##반환합니다. \n",
    "n_iter = 0\n",
    "\n",
    "for train_index, test_index in kfold.split(features):\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    pred = df_clf.predict(X_test)\n",
    "    n_iter += 1\n",
    "    \n",
    "    accuracy = np.round(accuracy_score(y_test, pred), 4)\n",
    "    train_size = X_train.shape[0]\n",
    "    test_size = X_test.shape[0]\n",
    "    print('\\n#{0} 교차 검증 정확도: {1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'.format(n_iter, accuracy, train_size, test_size))\n",
    "    print(\"\\n#{0} 검증 세트 인덱스: {1}\".format(n_iter, test_index))\n",
    "    cv_accuracy.append(accuracy)\n",
    "print(\"평균 정확됴: \", np.mean(cv_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Stratified K폴드\n",
    "'''\n",
    "Stratified K폴드는 불균형한 분포를 가진 데이터 집합을 위한 폴드 방식입니다.\n",
    "이때 특정 레이블 값이 특이하게 많거나 적어서 분포가 치우치는 것을 말합니다.\n",
    "이를 막기 위해 원본 데이터와 유사한 값의 분포를 학습/테스트 세트에도 유지하는 게 중요합니다.\n",
    "이를 StratifiedKFold를 통해 개선해 보도록 하겠습니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    50\n",
       "1    50\n",
       "0    50\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns = iris.feature_names)\n",
    "iris_df['label'] = iris.target\n",
    "iris_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 교차검증: 1\n",
      "학습 레이블 데이터 분포: \n",
      " 2    50\n",
      "1    50\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포: \n",
      " 0    50\n",
      "Name: label, dtype: int64\n",
      "## 교차검증: 2\n",
      "학습 레이블 데이터 분포: \n",
      " 2    50\n",
      "0    50\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포: \n",
      " 1    50\n",
      "Name: label, dtype: int64\n",
      "## 교차검증: 3\n",
      "학습 레이블 데이터 분포: \n",
      " 1    50\n",
      "0    50\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포: \n",
      " 2    50\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##이슈가 발생하는 현상 도출\n",
    "kfold = KFold(n_splits = 3)\n",
    "n_iter = 0\n",
    "for train_index, test_index in kfold.split(iris_df):\n",
    "    n_iter += 1\n",
    "    label_train = iris_df['label'].iloc[train_index]\n",
    "    label_test = iris_df['label'].iloc[test_index]\n",
    "    print(\"## 교차검증: {0}\".format(n_iter))\n",
    "    print(\"학습 레이블 데이터 분포: \\n\", label_train.value_counts())\n",
    "    print(\"검증 레이블 데이터 분포: \\n\", label_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "결과를 보면 학습 레이블과 검증 레이블이 완전히 다른 값으로 추출되었습니다. 따라서 여기서는 KFold가 아닌, \n",
    "StratifiedKFold를 사용해야 합니다. 이때는 split 함수에 인자로 피처 데이터 뿐만 아니라 레이블 데이터도 들어갑니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 교차검증: 1\n",
      "학습 레이블 데이터 분포: \n",
      " 2    34\n",
      "1    33\n",
      "0    33\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포: \n",
      " 1    17\n",
      "0    17\n",
      "2    16\n",
      "Name: label, dtype: int64\n",
      "## 교차검증: 2\n",
      "학습 레이블 데이터 분포: \n",
      " 1    34\n",
      "2    33\n",
      "0    33\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포: \n",
      " 2    17\n",
      "0    17\n",
      "1    16\n",
      "Name: label, dtype: int64\n",
      "## 교차검증: 3\n",
      "학습 레이블 데이터 분포: \n",
      " 0    34\n",
      "2    33\n",
      "1    33\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포: \n",
      " 2    17\n",
      "1    17\n",
      "0    16\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 3)\n",
    "n_iter = 0\n",
    "\n",
    "for train_index, test_index in skf.split(iris_df, iris_df['label']):\n",
    "    n_iter += 1\n",
    "    label_train = iris_df['label'].iloc[train_index]\n",
    "    label_test = iris_df['label'].iloc[test_index]\n",
    "    print(\"## 교차검증: {0}\".format(n_iter))\n",
    "    print(\"학습 레이블 데이터 분포: \\n\", label_train.value_counts())\n",
    "    print(\"검증 레이블 데이터 분포: \\n\", label_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#1 교차 검증 정확도: 1.0, 학습 데이터 크기: 100, 검증 데이터 크기: 50\n",
      "\n",
      "#1 검증 세트 인덱스: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  50\n",
      "  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66 100 101\n",
      " 102 103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "\n",
      "#2 교차 검증 정확도: 0.98, 학습 데이터 크기: 100, 검증 데이터 크기: 50\n",
      "\n",
      "#2 검증 세트 인덱스: [ 17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  67\n",
      "  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82 116 117 118\n",
      " 119 120 121 122 123 124 125 126 127 128 129 130 131 132]\n",
      "\n",
      "#3 교차 검증 정확도: 1.0, 학습 데이터 크기: 100, 검증 데이터 크기: 50\n",
      "\n",
      "#3 검증 세트 인덱스: [ 34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  83  84\n",
      "  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 133 134 135\n",
      " 136 137 138 139 140 141 142 143 144 145 146 147 148 149]\n",
      "[1.0, 0.98, 1.0]\n",
      "평균 정확도:  0.9933333333333333\n"
     ]
    }
   ],
   "source": [
    "###위 결과를 보시면 데이터 분할이 잘 된 것을 확인할 수 있습니다. 이를 이용하여 붓꽃 데이터를 교차검증 하도록 하겠습니다.\n",
    "dt_clf = DecisionTreeClassifier(random_state = 156)\n",
    "\n",
    "skfold = StratifiedKFold(n_splits = 3)\n",
    "n_iter= 0\n",
    "cv_accuracy = []\n",
    "\n",
    "for train_index, test_index in skfold.split(features, label):\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "    \n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    pred = df_clf.predict(X_test)\n",
    "    \n",
    "    n_iter += 1\n",
    "    \n",
    "    accuracy = np.round(accuracy_score(pred, y_test), 4)\n",
    "    train_size = X_train.shape[0]\n",
    "    test_size = X_test.shape[0]\n",
    "    print('\\n#{0} 교차 검증 정확도: {1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'.format(n_iter, accuracy, train_size, test_size))\n",
    "    print(\"\\n#{0} 검증 세트 인덱스: {1}\".format(n_iter, test_index))\n",
    "    cv_accuracy.append(accuracy)\n",
    "print(cv_accuracy)\n",
    "print(\"평균 정확도: \", np.mean(cv_accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##위 폴딩 알고리즘을 하나로 압축한 함수가 cross_val_score()입니다. \n",
    "##주요 파라미터로는 estimator, X,y, scoring, cv입니다.\n",
    "##estimator는 회귀인지 분류인지, X는 피처 데이터, y는 레이블 데이터, cv는 교차 검증 폴드 수 입니다.\n",
    "##scoring는 예측 성능 평가 지표를 기술합니다.\n",
    "##함수의 반환값은 scoring에 지정된 성능 지표 측정값을 배열 형태로 반황합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "교차 검증별 정확도:  [0.98 0.94 0.98]\n",
      "평균 검증 정확도: 0.9667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris_data = load_iris()\n",
    "df_clf = DecisionTreeClassifier(random_state = 156)\n",
    "\n",
    "data = iris_data.data\n",
    "label = iris_data.target\n",
    "\n",
    "scores = cross_val_score(df_clf, data, label, cv = 3, scoring=\"accuracy\")\n",
    "print(\"교차 검증별 정확도: \", np.round(scores, 4))\n",
    "print(\"평균 검증 정확도:\", np.round(np.mean(scores),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "데이터 전처리는 머신러닝 알고리즘만큼 중요합니다.\n",
    "사이킷런의 머신러닝 알고리즘을 적용하기 위해서는 데이터에 미리 처리해야 할 기본 사항이 있습니다.\n",
    "첫번째로 결손값을 허용하지 않습니다. 따라서 결손값은 피처의 평균값으로 대체하거나 아얘 드롭, 또는 데이터를 잘 반영할 수 있는 값으로\n",
    "대체되어야 합니다.\n",
    "두번째는 문자열 값 입력을 허용하지 않습니다. 따라서 모든 문자열 데이터를 숫자로 바꿔야합니다. \n",
    "\n",
    "대표적인 인코딩 방식은 레이블 인코딩과 원-핫 인코딩입니다.\n",
    "\n",
    "레이블 인코딩은 카테고리 피처를 숫자 값으로 변환하는 것입니다.\n",
    "레이블 인코딩은 LabelEncoder클래스를 통해 구현합니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코딩 변환 값:  [0 1 4 5 3 3 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "items = ['TV', '냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서','믹서']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(items)\n",
    "labels = encoder.transform(items)\n",
    "print(\"인코딩 변환 값: \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TV' '냉장고' '믹서' '선풍기' '전자레인지' '컴퓨터']\n"
     ]
    }
   ],
   "source": [
    "##어떤 문자열이 어떤 숫자로 변했는지 알기 위해 클래스 속성인 classes_값을 확인하면 됩니다.\n",
    "print(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['전자레인지' '컴퓨터' '믹서' 'TV' '냉장고' '냉장고' '선풍기' '선풍기']\n"
     ]
    }
   ],
   "source": [
    "##inverse_transform함수를 통해 다시 디코딩할 수 있습니다.\n",
    "print(encoder.inverse_transform([4,5,2,0,1,1,3,3]))\n",
    "'''\n",
    "레이블 인코딩의 문제점은 숫자 값의 크고 작음에 대한 특성이 작용하여 예측 성능이 떨어질 수 있습니다.\n",
    "트리 계열의 알고리즘에서는 숫자의 이런 특성을 반영하지 않지만, 회귀와 같은 알고리즘에서는 적용하지 않아야 합니다.\n",
    "이러한 문제점을 해결한 인코딩 방식이 원-핫 인코딩입니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "원-핫 인코딩은 고유 값에 해당하는 칼럼에만 1을 표시하고 나머지는 0을 표시하느 방법입니다.\n",
    "OneHotEncoder클래스를 통해 구현할 수 있습니다.\n",
    "여기서 주의할점은 모든 문자열 값이 숫자형 값으로 변환되어야 하며, 입력 값으로 2차원 데이터를 받아야합니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [4]\n",
      " [5]\n",
      " [3]\n",
      " [3]\n",
      " [2]\n",
      " [2]]\n",
      "\n",
      "\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "items = ['TV', '냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서','믹서']\n",
    "\n",
    "##우선 문자열을 숫자로 바꿔야 합니다. 따라서 앞에서 다뤘던 레이블 인코딩을 먼저 해줍니다.\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(items)\n",
    "labels = encoder.transform(items)\n",
    "\n",
    "##2차원 데이터로 바꿔야합니다.\n",
    "labels = labels.reshape(-1,1)\n",
    "print(labels)\n",
    "print('\\n')\n",
    "oh_encoder = OneHotEncoder()\n",
    "oh_encoder.fit(labels)\n",
    "oh_labels = oh_encoder.transform(labels)\n",
    "print(oh_labels.toarray())\n",
    "\n",
    "##판다스에서는 get_dummies를 통해 굳이 문자열을 숫자형으로 바꾸지 않고 바로 원핫인코딩 할 수 있는 함수가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "서로 다른 변수의 값 변위를 일정한 수준으로 맞추는 작업을 피처 스케일링이라고 합니다.\n",
    "표준화와 정규화가 있습니다.\n",
    "표준화는 평균이 0, 분산이 1인 정규 분포를 가진 값으로 변환하는 것이고,\n",
    "정규화는 서로 다른 피처의 크기를 통일하기 위해 크기를 변환해주는 개념입니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  sepal length (cm)    5.843333\n",
      "sepal width (cm)     3.057333\n",
      "petal length (cm)    3.758000\n",
      "petal width (cm)     1.199333\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "var:  sepal length (cm)    0.685694\n",
      "sepal width (cm)     0.189979\n",
      "petal length (cm)    3.116278\n",
      "petal width (cm)     0.581006\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "##StandardScaler는 표준화를 지원하는 클래스입니다. \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "iris_data = iris.data\n",
    "iris_df = pd.DataFrame(data = iris_data, columns = iris.feature_names)\n",
    "\n",
    "print('mean: ', iris_df.mean())\n",
    "print(\"\\n\")\n",
    "print(\"var: \", iris_df.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm)   -1.690315e-15\n",
      "sepal width (cm)    -1.842970e-15\n",
      "petal length (cm)   -1.698641e-15\n",
      "petal width (cm)    -1.409243e-15\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "sepal length (cm)    1.006711\n",
      "sepal width (cm)     1.006711\n",
      "petal length (cm)    1.006711\n",
      "petal width (cm)     1.006711\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(iris_df)\n",
    "iris_scaled = scaler.transform(iris_df)\n",
    "\n",
    "iris_df_scaled = pd.DataFrame(data = iris_scaled, columns = iris.feature_names)\n",
    "print(iris_df_scaled.mean())\n",
    "print(\"\\n\")\n",
    "print(iris_df_scaled.var())\n",
    "##결과를 보시면 평균이 0 분산이 1에 가까운 값으로 변한것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min:  sepal length (cm)    0.0\n",
      "sepal width (cm)     0.0\n",
      "petal length (cm)    0.0\n",
      "petal width (cm)     0.0\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "max:  sepal length (cm)    1.0\n",
      "sepal width (cm)     1.0\n",
      "petal length (cm)    1.0\n",
      "petal width (cm)     1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "##MinMaxScaler클래스를 통해 데이터 값을 0~1로 변환할 수 있습니다.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(iris_df)\n",
    "iris_scaler = scaler.transform(iris_df)\n",
    "iris_df_scaled = pd.DataFrame(data = iris_scaler, columns = iris.feature_names)\n",
    "print(\"min: \", iris_df_scaled.min())\n",
    "print(\"\\n\")\n",
    "print(\"max: \", iris_df_scaled.max())\n",
    "\n",
    "##결과를 보시면 최소값이 0, 최대값이 1로, 0~1사이의 값으로 변한 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
